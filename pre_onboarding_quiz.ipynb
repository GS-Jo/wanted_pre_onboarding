{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre_onboarding_quiz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EhgDxlCHPlx",
        "outputId": "80725968-bdf8-41bf-ee38-d0ec4016bc9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyRJytgjHUn_",
        "outputId": "c870b238-d45a-4aa4-84eb-c328b2d11ce6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6h9n3tuxHVMK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 1"
      ],
      "metadata": {
        "id": "0J9uS-B-HnwB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    # 문제 1-1\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    symbols = '[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]'\n",
        "\n",
        "    for sentence in sequences:\n",
        "      words = [word.lower() for word in word_tokenize(sentence)]\n",
        "      words = [word for word in words if word not in symbols]\n",
        "      result.append(words)\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "\n",
        "    # 문제 1-2\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    tokenized_list = self.preprocessing(sequences)\n",
        "    tokens = sum(tokenized_list, [])\n",
        "    tokens = list(OrderedDict.fromkeys(tokens))\n",
        "\n",
        "    for word in tokens:\n",
        "      if self.word_dict.get(word)==None:\n",
        "        self.word_dict[word]=len(self.word_dict)\n",
        "\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "\n",
        "      # 문제 1-3\n",
        "      for one_list in tokens:\n",
        "        indexing = [self.word_dict.get(word) if word in self.word_dict else self.word_dict.get('oov') for word in one_list]\n",
        "        result.append(indexing)\n",
        "\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "PvxXHrqOHY7i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### test\n",
        "input_list = ['I go to school.','I LIKE pizza!']\n",
        "test_input = ['You are So beautiful.','hey guys! you love pizza?']"
      ],
      "metadata": {
        "id": "8w4gspzeHqog"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Tokenizer()"
      ],
      "metadata": {
        "id": "B0iurbrlHx7Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.fit_transform(input_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ech_nlQPHzSR",
        "outputId": "3dc40008-ff8b-48a5-a229-db229194ffae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ktwhny9Hb57S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 2"
      ],
      "metadata": {
        "id": "NcLIEYtvIvyd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer():\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "    # tfidf_matrix 선언\n",
        "    self.tfidf_matrix = []\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    \n",
        "    # 문제 2-1\n",
        "\n",
        "    n = len(tokenized)\n",
        "    tokens = list(set(sum(tokenized, [])))\n",
        "\n",
        "    import scipy as sp\n",
        "\n",
        "    #idf행렬 만들기\n",
        "    idf_matrix = []\n",
        "    for t in tokens:\n",
        "      df = len([doc for doc in tokenized if t in doc])\n",
        "      idf = sp.log(n/float(1+df))\n",
        "      idf_matrix.append(idf)\n",
        "\n",
        "    self.fit_checker = True\n",
        "    return idf_matrix\n",
        "\n",
        "  def transform(self, sequences):\n",
        "\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      \n",
        "      # 문제 2-2\n",
        "\n",
        "      #idf행렬 불러오기\n",
        "      idf_mat = self.fit(sequences)\n",
        "      tokens = list(set(sum(tokenized, [])))\n",
        "\n",
        "      # tf행렬 만들기\n",
        "      tf_mat = []\n",
        "      for idx in range(len(tokenized)):\n",
        "        doc = tokenized[idx]\n",
        "        tf = [doc.count(t) for t in tokens]\n",
        "        tf_mat.append(tf)\n",
        "\n",
        "      # tf-idf = tf * idf\n",
        "      for tf_one in tf_mat:\n",
        "        multi = [tf_one[idx] * idf_mat[idx] for idx in range(len(tf_one))]\n",
        "        self.tfidf_matrix.append(multi)\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "TuMgmUyab46E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트\n",
        "test_list = ['I go to school.','I LIKE pizza!','You know you are So beautiful.','hey guys! you love pizza?']"
      ],
      "metadata": {
        "id": "khFGLzfpjimA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(Tokenizer())"
      ],
      "metadata": {
        "id": "1xYvz4E2jqEq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#idf행렬\n",
        "tfidf.fit(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUEXhRWSkDIj",
        "outputId": "29ce93b7-5ae6-49de-c5b5-befc55ea81ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28768207245178085,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.28768207245178085,\n",
              " 0.28768207245178085,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf값\n",
        "tfidf.fit_transform(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkem-b3j_QN",
        "outputId": "3924dfa8-3927-4afe-b26c-ffdf0704ea31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5753641449035617,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mGsudHy9kGl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}