{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre_onboarding_quiz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EhgDxlCHPlx",
        "outputId": "243da766-4121-4dfb-8bca-b16307fbd292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyRJytgjHUn_",
        "outputId": "7dbf41bd-0c89-444a-d806-b6de4a38bc9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6h9n3tuxHVMK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 1"
      ],
      "metadata": {
        "id": "0J9uS-B-HnwB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    # 문제 1-1\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    symbols = '[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]'\n",
        "\n",
        "    for sentence in sequences:\n",
        "      words = [word.lower() for word in word_tokenize(sentence)]\n",
        "      words = [word for word in words if word not in symbols]\n",
        "      result.append(words)\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "\n",
        "    # 문제 1-2\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    tokenized_list = self.preprocessing(sequences)\n",
        "    tokens = sum(tokenized_list, [])\n",
        "    tokens = list(OrderedDict.fromkeys(tokens))\n",
        "\n",
        "    for word in tokens:\n",
        "      if self.word_dict.get(word)==None:\n",
        "        self.word_dict[word]=len(self.word_dict)\n",
        "\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "\n",
        "      # 문제 1-3\n",
        "      for one_list in tokens:\n",
        "        indexing = [self.word_dict.get(word) if word in self.word_dict else self.word_dict.get('oov') for word in one_list]\n",
        "        result.append(indexing)\n",
        "\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "PvxXHrqOHY7i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### test\n",
        "input_list = ['I go to school.','I LIKE pizza!']\n",
        "test_input = ['You are So beautiful.','hey guys! you love pizza?']"
      ],
      "metadata": {
        "id": "8w4gspzeHqog"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Tokenizer()"
      ],
      "metadata": {
        "id": "B0iurbrlHx7Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.fit_transform(input_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ech_nlQPHzSR",
        "outputId": "9f812436-afef-4c44-a10b-0e95b516d6d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ktwhny9Hb57S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 2"
      ],
      "metadata": {
        "id": "NcLIEYtvIvyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer():\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    \n",
        "    # 문제 2-1\n",
        "\n",
        "    n = len(tokenized)\n",
        "    tokens = list(set(sum(tokenized, [])))\n",
        "\n",
        "    import scipy as sp\n",
        "\n",
        "    idf_matrix = []\n",
        "    for t in tokens:\n",
        "      df = len([doc for doc in tokenized if t in doc])\n",
        "      idf = sp.log(n/float(1+df))\n",
        "      idf_matrix.append(idf)\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "\n",
        "    if self.fit_checker:\n",
        "\n",
        "\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      \n",
        "      # 문제 2-2\n",
        "      idf_mat = self.fit(sequences)\n",
        "\n",
        "      tf_mat = []\n",
        "      for idx in range(len(tokenized)):\n",
        "        doc = tokenized[idx]\n",
        "        tf = [doc.count(t) for t in tokens]\n",
        "        tf_mat.append(tf)\n",
        "\n",
        "      tfidf_matrix = []\n",
        "      for one in tf_mat:\n",
        "        multi = [tf[idx] * idf_mat[idx] for idx in range(len(one))]\n",
        "        tfidf_matrix.append(multi)\n",
        "\n",
        "        return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "TuMgmUyab46E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}