{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre_onboarding_quiz_cho.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EhgDxlCHPlx",
        "outputId": "eb00ae7d-4fed-4d38-a889-ed3f734aa0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyRJytgjHUn_",
        "outputId": "0a7b6547-5650-46e6-9714-710b97287723"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6h9n3tuxHVMK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 1"
      ],
      "metadata": {
        "id": "0J9uS-B-HnwB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "\n",
        "    # 문제 1-1\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    import re\n",
        "\n",
        "    for sentence in sequences:\n",
        "\n",
        "      # 특수문자 제거\n",
        "      sentence = re.sub('[-=+,#/\\?:^.@*\\\"※~ㆍ!』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·]','',sentence)\n",
        "      # 대문자 소문자화\n",
        "      words = [word.lower() for word in word_tokenize(sentence)]\n",
        "      result.append(words)\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "\n",
        "    # 문제 1-2\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    # 토큰화\n",
        "    tokenized_list = self.preprocessing(sequences)\n",
        "    #리스트 내 list 벗기기\n",
        "    tokens = sum(tokenized_list, [])\n",
        "  \n",
        "    # 리스트 내 중복 토큰 제거한 말뭉치 생성\n",
        "    for word in tokens:\n",
        "      if self.word_dict.get(word)==None:\n",
        "        self.word_dict[word]=len(self.word_dict)\n",
        "\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "\n",
        "      # 문제 1-3\n",
        "\n",
        "      # 말뭉치를 활용해 문장별 정수로 인덱싱\n",
        "      for one_sentence in tokens:\n",
        "        int_indexing = [self.word_dict.get(word) if word in self.word_dict \\\n",
        "                    else self.word_dict.get('oov') for word in one_sentence]\n",
        "        result.append(int_indexing)\n",
        "\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "PvxXHrqOHY7i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### test\n",
        "input_list = ['I go to school.','I LIKE pizza!']\n",
        "test_input = [\"You are So beautiful.\",\"i can't make it\"]"
      ],
      "metadata": {
        "id": "8w4gspzeHqog"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()"
      ],
      "metadata": {
        "id": "7fDbI88ZZJz8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.fit_transform(input_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ech_nlQPHzSR",
        "outputId": "15151444-46cc-4631-c75a-7984fd6f8818"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ktwhny9Hb57S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 2"
      ],
      "metadata": {
        "id": "NcLIEYtvIvyd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer():\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "    \n",
        "    \n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    \n",
        "    # 문제 2-1\n",
        "    import scipy as sp\n",
        "\n",
        "    idf_matrix = []\n",
        "    # 전체 문장의 갯수\n",
        "    N = len(tokenized)\n",
        "    # unique한 단어 뭉치 만들기\n",
        "    tokens = list(set(sum(tokenized, [])))\n",
        "\n",
        "    for t in tokens:\n",
        "      # 단어별 df값 구하기\n",
        "      df = len([doc for doc in tokenized if t in doc])\n",
        "      # 식을 이용해 단어별 idf값 구하기\n",
        "      idf = sp.log(int(N)/float(1+df))\n",
        "      idf_matrix.append(idf)\n",
        "\n",
        "    self.fit_checker = True\n",
        "    return idf_matrix\n",
        "    \n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      \n",
        "      # 문제 2-2\n",
        "\n",
        "      idf_matrix = self.fit(sequences)\n",
        "      N = len(tokenized)\n",
        "      tokens = list(set(sum(tokenized, [])))\n",
        "      \n",
        "\n",
        "      #tf행렬 만들기\n",
        "      tf_matrix = []\n",
        "\n",
        "      for idx in range(N):\n",
        "        sentence = tokenized[idx]\n",
        "        tf = [sentence.count(t) for t in tokens]\n",
        "        tf_matrix.append(tf)\n",
        "\n",
        "      # tf-idf행렬은  tf x idf\n",
        "      self.tfidf_matrix = []\n",
        "\n",
        "      for tf in tf_matrix:\n",
        "        tfidf = [tf[idx] * idf_matrix[idx] for idx in range(len(tf))]\n",
        "        self.tfidf_matrix.append(tfidf)\n",
        "\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "TuMgmUyab46E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트\n",
        "test_list = ['I go to school.','I LIKE pizza!','You know you are So beautiful.','hey guys! you love pizza?']"
      ],
      "metadata": {
        "id": "khFGLzfpjimA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(Tokenizer())"
      ],
      "metadata": {
        "id": "1xYvz4E2jqEq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#idf행렬\n",
        "tfidf.fit(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUEXhRWSkDIj",
        "outputId": "c51b4696-daa9-4bb2-9e1a-dbad4dee0bda"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28768207245178085,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.28768207245178085,\n",
              " 0.28768207245178085,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453,\n",
              " 0.6931471805599453]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf값\n",
        "tfidf.fit_transform(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkem-b3j_QN",
        "outputId": "5ac0d13a-3a97-4703-c9e1-fa2f76ce184e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5753641449035617,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mGsudHy9kGl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}